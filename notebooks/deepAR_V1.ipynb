{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76210a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m_sam\\anaconda3\\envs\\ts-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-03 03:59:41,549\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-07-03 03:59:41,696\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# al tope de tu notebook, _antes_ de cualquier otra importación\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "from pytorch_lightning import Trainer\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import torch\n",
    "from gluonts.torch.distributions import StudentTOutput\n",
    "from ray.air import session\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9bab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m_sam\\AppData\\Local\\Temp\\ipykernel_50596\\281954699.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  sell_in = pd.read_csv('../data/sell-in.txt', sep='\\t', parse_dates=['periodo'])\n"
     ]
    }
   ],
   "source": [
    "# 1) load\n",
    "sell_in = pd.read_csv('../data/sell-in.txt', sep='\\t', parse_dates=['periodo'])\n",
    "prod_vigentes = pd.read_csv('../data/product_id_apredecir201912.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa4e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Convierte 'periodo' a datetime (si viene en formato YYYYMM)\n",
    "sell_in['periodo'] = pd.to_datetime(\n",
    "    sell_in['periodo'],\n",
    "    format='%Y%m'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64420ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Agrega tn a nivel (periodo, product_id)\n",
    "sell_in_agg = (\n",
    "    sell_in\n",
    "    .groupby(['periodo', 'product_id'], as_index=False)['tn']\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb78e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_in_agg['periodo'] = (\n",
    "    pd.to_datetime(sell_in_agg['periodo'], format=\"%Y-%m-%d\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9961348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) constants\n",
    "FREQ = \"M\"\n",
    "PREDICTION_LENGTH = 2\n",
    "NUM_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162f0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Construye dataset + lista de pids\n",
    "dataset = []\n",
    "pids = []\n",
    "for pid in prod_vigentes['product_id'].unique():\n",
    "    # a) filtrar\n",
    "    df = sell_in_agg.loc[\n",
    "        sell_in_agg['product_id'] == pid,\n",
    "        ['periodo', 'tn']\n",
    "    ].set_index('periodo').sort_index()\n",
    "\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # b) remuestreo mensual sumando todas las ventas de cada mes\n",
    "    monthly = df['tn'].resample(FREQ).sum()\n",
    "\n",
    "    # c) rellenar meses sin datos con 0\n",
    "    target  = monthly.fillna(0).values\n",
    "\n",
    "    # d) armar la entrada del ListDataset\n",
    "    dataset.append({\n",
    "        \"start\": monthly.index[0],          # Timestamp('2017-01-31 00:00:00')\n",
    "        \"target\": target.astype('float32'),\n",
    "        \"item_id\": str(pid),\n",
    "    })\n",
    "    pids.append(pid)\n",
    "\n",
    "\n",
    "train_ds = ListDataset(dataset, freq=FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00353295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 03:45:58,806\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-07-03 03:48:46</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:47.98        </td></tr>\n",
       "<tr><td>Memory:      </td><td>28.5/63.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 20.000: None | Iter 5.000: None<br>Logical resource usage: 2.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">  hidden_size</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  num_layers</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  MAE</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_deepar_60923_00000</td><td>TERMINATED</td><td>127.0.0.1:47208</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">           40</td><td style=\"text-align: right;\">0.00546043 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         93.1867</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00001</td><td>TERMINATED</td><td>127.0.0.1:50376</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">           40</td><td style=\"text-align: right;\">0.000341299</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         83.7926</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00002</td><td>TERMINATED</td><td>127.0.0.1:44964</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.00537067 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         28.3956</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00003</td><td>TERMINATED</td><td>127.0.0.1:15000</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.00802639 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.049 </td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00004</td><td>TERMINATED</td><td>127.0.0.1:33832</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.00229266 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         92.6292</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00005</td><td>TERMINATED</td><td>127.0.0.1:48140</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">           40</td><td style=\"text-align: right;\">0.00018712 </td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         98.4608</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00006</td><td>TERMINATED</td><td>127.0.0.1:39164</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.000181918</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         56.3813</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00007</td><td>TERMINATED</td><td>127.0.0.1:31680</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.000288027</td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         32.0246</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00008</td><td>TERMINATED</td><td>127.0.0.1:39920</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.000157193</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         87.5006</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00009</td><td>TERMINATED</td><td>127.0.0.1:29052</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.000224093</td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.9566</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00010</td><td>TERMINATED</td><td>127.0.0.1:30380</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.00393586 </td><td style=\"text-align: right;\">           3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.3474</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00011</td><td>TERMINATED</td><td>127.0.0.1:21000</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.00320422 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         57.7685</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00012</td><td>TERMINATED</td><td>127.0.0.1:22220</td><td style=\"text-align: right;\">      10</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.0062303  </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         32.867 </td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00013</td><td>TERMINATED</td><td>127.0.0.1:42544</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           20</td><td style=\"text-align: right;\">0.00013613 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         49.4218</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00014</td><td>TERMINATED</td><td>127.0.0.1:22884</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           80</td><td style=\"text-align: right;\">0.00541573 </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         48.6469</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00015</td><td>TERMINATED</td><td>127.0.0.1:41352</td><td style=\"text-align: right;\">      20</td><td style=\"text-align: right;\">           40</td><td style=\"text-align: right;\">0.00459524 </td><td style=\"text-align: right;\">           2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         49.4109</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  MAE</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_deepar_60923_00000</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00001</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00002</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00003</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00004</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00005</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00006</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00007</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00008</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00009</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00010</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00011</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00012</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00013</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00014</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "<tr><td>train_deepar_60923_00015</td><td style=\"text-align: right;\">  nan</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 03:48:46,996\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to 'C:/Users/m_sam/ray_results/train_deepar_2025-07-03_03-45-58' in 0.0361s.\n",
      "2025-07-03 03:48:47,034\tINFO tune.py:1048 -- Total run time: 168.23 seconds (167.95 seconds for the tuning loop).\n",
      "2025-07-03 03:48:47,054\tWARNING experiment_analysis.py:568 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración: None\n"
     ]
    }
   ],
   "source": [
    "# def train_deepar(config):\n",
    "#     # 1) Construye el estimador con los hiperparámetros que quieres\n",
    "#     estimator = DeepAREstimator(\n",
    "#         freq=\"M\",\n",
    "#         prediction_length=1,\n",
    "#         # input_size=1,\n",
    "#         hidden_size=config[\"hidden_size\"],\n",
    "#         num_layers=config[\"num_layers\"],\n",
    "#         lr=config[\"lr\"],    \n",
    "#         num_batches_per_epoch=config[\"epochs\"],\n",
    "#         batch_size=32,  # o también tunéalo si quieres\n",
    "#         distr_output=StudentTOutput(),  # o la distribución que uses\n",
    "#         scaling=True,\n",
    "#     )\n",
    "#     predictor = estimator.train(train_ds)\n",
    "#     test_ds = ListDataset(\n",
    "#         [\n",
    "#             {\n",
    "#                 \"start\": item[\"start\"],\n",
    "#                 # dejamos sólo la parte 'histórica' y eliminamos las últimas `PREDICTION_LENGTH` observaciones\n",
    "#                 \"target\": item[\"target\"][:-PREDICTION_LENGTH],\n",
    "#                 \"item_id\": item[\"item_id\"],\n",
    "#             }\n",
    "#             for item in dataset  # dataset debe ser la lista de diccionarios que usaste para train_ds\n",
    "#         ],\n",
    "#         freq=FREQ,\n",
    "#     )\n",
    "#     forecast_it, ts_it = make_evaluation_predictions(\n",
    "#         dataset=test_ds,\n",
    "#         predictor=predictor,\n",
    "#         num_samples=100,\n",
    "#     )\n",
    "#     evaluator   = Evaluator(quantiles=[0.5])\n",
    "#     agg_metrics, _ = evaluator(ts_it, forecast_it)\n",
    "#     forecasts = list(forecast_it)\n",
    "#     tss       = list(ts_it)\n",
    "    \n",
    "#     # 3) compute MAE for each series, then average\n",
    "#     maes = []\n",
    "#     for ts, fcst in zip(tss, forecasts):\n",
    "#         y_true = ts[-PREDICTION_LENGTH:]\n",
    "#         # fcst.samples.shape == (num_samples, prediction_length)\n",
    "#         y_pred = np.mean(fcst.samples, axis=0)\n",
    "#         maes.append(mean_absolute_error(y_true, y_pred))\n",
    "    \n",
    "#     mean_mae = float(np.mean(maes))\n",
    "    \n",
    "#     # 4) report back to Tune\n",
    "#     # tune.track.log(MAE=mean_mae)\n",
    "#     # tune.report(MAE=mean_mae)\n",
    "#     session.report({\"MAE\": mean_mae})\n",
    "    \n",
    "\n",
    "# # define espacio de búsqueda\n",
    "# search_space = {\n",
    "#     \"lr\":        tune.loguniform(1e-4, 1e-2),\n",
    "#     \"epochs\":    tune.choice([10, 20, 30]),\n",
    "#     \"num_layers\":tune.choice([1, 2, 3]),\n",
    "#     \"hidden_size\":tune.choice([20, 40, 80]),\n",
    "# }\n",
    "\n",
    "# scheduler = ASHAScheduler(\n",
    "#     # metric=\"MAE\",\n",
    "#     # mode=\"min\",\n",
    "#     max_t=30,\n",
    "#     grace_period=5,\n",
    "# )\n",
    "\n",
    "# # 2) indícale a Ray un directorio base MUY corto\n",
    "# local_dir = r\"C:\\tmp\\ray\"\n",
    "# os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# # 3) crea nombres de sub-carpeta super-cortos para cada trial\n",
    "# def short_dirname_creator(trial):\n",
    "#     return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "# # 4) lanza el hyper-param search usando esas dos opciones\n",
    "# analysis = tune.run(\n",
    "#     train_deepar,\n",
    "#     config=search_space,\n",
    "#     num_samples=16,\n",
    "#     scheduler=scheduler,\n",
    "#     resources_per_trial={\"cpu\": 2},\n",
    "#     local_dir=local_dir,                        # aquí\n",
    "#     trial_dirname_creator=short_dirname_creator, # y aquí\n",
    "#     metric=\"MAE\",      # ← the name you used in tune.report(...)\n",
    "#     mode=\"min\",        # ← \"min\" for MAE, \"max\" for e.g. accuracy\n",
    "# )\n",
    "\n",
    "# print(\"Mejor configuración:\", analysis.best_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af546a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 04:24:17,636\tINFO tune.py:613 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">     MAE</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_deepar_bac78_00000</td><td style=\"text-align: right;\">11.5246 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00001</td><td style=\"text-align: right;\">11.3448 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00002</td><td style=\"text-align: right;\">10.5293 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00003</td><td style=\"text-align: right;\">14.0623 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00004</td><td style=\"text-align: right;\">10.135  </td></tr>\n",
       "<tr><td>train_deepar_bac78_00005</td><td style=\"text-align: right;\">12.7884 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00006</td><td style=\"text-align: right;\">10.7748 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00007</td><td style=\"text-align: right;\"> 9.89062</td></tr>\n",
       "<tr><td>train_deepar_bac78_00008</td><td style=\"text-align: right;\">11.4331 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00009</td><td style=\"text-align: right;\"> 9.2675 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00010</td><td style=\"text-align: right;\">11.6333 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00011</td><td style=\"text-align: right;\"> 9.97752</td></tr>\n",
       "<tr><td>train_deepar_bac78_00012</td><td style=\"text-align: right;\"> 9.57836</td></tr>\n",
       "<tr><td>train_deepar_bac78_00013</td><td style=\"text-align: right;\">11.1463 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00014</td><td style=\"text-align: right;\"> 9.1835 </td></tr>\n",
       "<tr><td>train_deepar_bac78_00015</td><td style=\"text-align: right;\">11.1687 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 04:25:43,957\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to 'C:/Users/m_sam/ray_results/train_deepar_2025-07-03_04-24-17' in 0.0321s.\n",
      "2025-07-03 04:25:43,995\tINFO tune.py:1048 -- Total run time: 86.36 seconds (86.27 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración encontrada: {'lr': 0.0001954017201375677, 'epochs': 30, 'num_layers': 1, 'hidden_size': 40}\n"
     ]
    }
   ],
   "source": [
    "def train_deepar(config, train_ds, raw_dataset):\n",
    "    # 1) construye el estimador usando los hyperparámetros del config\n",
    "    estimator = DeepAREstimator(\n",
    "        freq=FREQ,\n",
    "        prediction_length=PREDICTION_LENGTH,\n",
    "        distr_output=StudentTOutput(),\n",
    "        scaling=True,\n",
    "        # aquí mapeamos tu \"num_layers\" y \"hidden_size\" (en GluonTS se llama num_cells)\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        hidden_size=config[\"hidden_size\"],\n",
    "        trainer_kwargs={\n",
    "            \"max_epochs\": config[\"epochs\"],\n",
    "            # \"accelerator\": \"gpu\",\n",
    "            # \"devices\": 1,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # 2) entrena\n",
    "    predictor = estimator.train(train_ds)\n",
    "\n",
    "    # 3) prepara el test_ds truncando las últimas PREDICTION_LENGTH observaciones\n",
    "    test_ds = ListDataset(\n",
    "        [\n",
    "            {\n",
    "                \"start\": item[\"start\"],\n",
    "                \"target\": item[\"target\"][:-PREDICTION_LENGTH],\n",
    "                \"item_id\": item[\"item_id\"],\n",
    "            }\n",
    "            for item in raw_dataset\n",
    "        ],\n",
    "        freq=FREQ,\n",
    "    )\n",
    "\n",
    "    # 4) genera predicciones\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=test_ds,\n",
    "        predictor=predictor,\n",
    "        num_samples=100,\n",
    "    )\n",
    "\n",
    "    forecasts = list(forecast_it)\n",
    "    tss       = list(ts_it)\n",
    "\n",
    "    # 5) calcula MAE promedio\n",
    "    maes = []\n",
    "    for ts, fcst in zip(tss, forecasts):\n",
    "        y_true = ts[-PREDICTION_LENGTH:]\n",
    "        y_pred = np.mean(fcst.samples, axis=0)\n",
    "        maes.append(mean_absolute_error(y_true, y_pred))\n",
    "    mean_mae = float(np.mean(maes))\n",
    "\n",
    "    # 6) reporta el resultado a Ray Tune\n",
    "    session.report({\"MAE\": mean_mae})\n",
    "\n",
    "\n",
    "# ——— scheduler y espacio de búsqueda ———\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=30,\n",
    "    grace_period=5,\n",
    ")\n",
    "\n",
    "search_space = {\n",
    "    \"lr\":          tune.loguniform(1e-4, 1e-2),\n",
    "    \"epochs\":      tune.choice([10, 20, 30]),\n",
    "    \"num_layers\":  tune.choice([1, 2, 3]),\n",
    "    \"hidden_size\": tune.choice([20, 40, 80]),\n",
    "}\n",
    "\n",
    "# ——— directorio corto para logs de Ray ———\n",
    "local_dir = r\"C:\\tmp\\ray\"\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "def short_dirname_creator(trial):\n",
    "    return f\"t{trial.trial_id}\"\n",
    "\n",
    "\n",
    "# ——— envolvemos nuestra función para inyectar los datasets ———\n",
    "trainable = tune.with_parameters(\n",
    "    train_deepar,\n",
    "    train_ds=train_ds,\n",
    "    raw_dataset=dataset,\n",
    ")\n",
    "\n",
    "# ——— lanzamos la búsqueda ———\n",
    "analysis = tune.run(\n",
    "    trainable,                          # <- NOTA: pasamos aquí el objeto retornado por with_parameters\n",
    "    config=search_space,\n",
    "    num_samples=16,\n",
    "    scheduler=scheduler,\n",
    "    resources_per_trial={\"cpu\": 2},\n",
    "    local_dir=local_dir,\n",
    "    trial_dirname_creator=short_dirname_creator,\n",
    "    metric=\"MAE\",   # nombre del campo que reportamos\n",
    "    mode=\"min\",     # queremos minimizar MAE\n",
    ")\n",
    "\n",
    "print(\"Mejor configuración encontrada:\", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4368f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\m_sam\\anaconda3\\envs\\ts-venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:71: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type        | Params | In sizes | Out sizes  \n",
      "---------------------------------------------------------------\n",
      "0 | model | DeepARModel | 10.2 K | ?        | [1, 100, 2]\n",
      "---------------------------------------------------------------\n",
      "10.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.2 K    Total params\n",
      "0.041     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 50it [00:01, 46.53it/s, v_num=10, train_loss=4.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 4.10063 (best 4.10063), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 50it [00:01, 46.07it/s, v_num=10, train_loss=3.700]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 3.70361 (best 3.70361), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 50it [00:01, 44.61it/s, v_num=10, train_loss=3.460]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 3.45875 (best 3.45875), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 50it [00:01, 46.58it/s, v_num=10, train_loss=3.240]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 3.24372 (best 3.24372), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 50it [00:01, 45.84it/s, v_num=10, train_loss=3.260]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: : 50it [00:01, 46.72it/s, v_num=10, train_loss=3.110]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 3.11416 (best 3.11416), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: : 50it [00:01, 45.88it/s, v_num=10, train_loss=2.980]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' reached 2.98126 (best 2.98126), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: : 50it [00:01, 45.37it/s, v_num=10, train_loss=3.070]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: : 50it [00:01, 46.83it/s, v_num=10, train_loss=2.930]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' reached 2.92665 (best 2.92665), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=8-step=450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: : 50it [00:01, 46.76it/s, v_num=10, train_loss=3.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: : 50it [00:01, 46.10it/s, v_num=10, train_loss=2.940]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: : 50it [00:01, 43.02it/s, v_num=10, train_loss=2.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' reached 2.90112 (best 2.90112), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=11-step=600.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: : 50it [00:01, 46.45it/s, v_num=10, train_loss=3.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: : 50it [00:01, 46.17it/s, v_num=10, train_loss=2.960]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: : 50it [00:01, 46.55it/s, v_num=10, train_loss=2.950]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: : 50it [00:01, 44.45it/s, v_num=10, train_loss=2.900]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' reached 2.89722 (best 2.89722), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=15-step=800.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: : 50it [00:01, 45.29it/s, v_num=10, train_loss=2.870]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' reached 2.87393 (best 2.87393), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=16-step=850.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: : 50it [00:01, 47.77it/s, v_num=10, train_loss=2.850]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' reached 2.84581 (best 2.84581), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=17-step=900.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: : 50it [00:01, 47.37it/s, v_num=10, train_loss=2.890]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: : 50it [00:01, 44.06it/s, v_num=10, train_loss=2.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' reached 2.80410 (best 2.80410), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=19-step=1000.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: : 50it [00:01, 43.97it/s, v_num=10, train_loss=2.800]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' reached 2.79575 (best 2.79575), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=20-step=1050.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: : 50it [00:01, 43.51it/s, v_num=10, train_loss=2.810]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: : 50it [00:01, 43.84it/s, v_num=10, train_loss=2.920]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: : 50it [00:01, 46.78it/s, v_num=10, train_loss=2.880]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: : 50it [00:01, 45.67it/s, v_num=10, train_loss=2.860]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: : 50it [00:01, 44.35it/s, v_num=10, train_loss=2.880]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: : 50it [00:01, 45.00it/s, v_num=10, train_loss=2.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' reached 2.69485 (best 2.69485), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=26-step=1350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: : 50it [00:01, 45.24it/s, v_num=10, train_loss=2.720]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: : 50it [00:01, 45.36it/s, v_num=10, train_loss=2.820]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: : 50it [00:01, 45.14it/s, v_num=10, train_loss=2.630]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 1500: 'train_loss' reached 2.63126 (best 2.63126), saving model to 'c:\\\\Users\\\\m_sam\\\\OneDrive\\\\Documentos\\\\Maestria\\\\LaboIII\\\\TP_Final\\\\lab3\\\\notebooks\\\\lightning_logs\\\\version_10\\\\checkpoints\\\\epoch=29-step=1500.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: : 50it [00:01, 44.67it/s, v_num=10, train_loss=2.630]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m_sam\\anaconda3\\envs\\ts-venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE final: 11.578603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Some of the requested item metrics are missing.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Si quieres métricas más completas:\u001b[39;00m\n\u001b[0;32m     63\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(quantiles\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.9\u001b[39m])\n\u001b[1;32m---> 64\u001b[0m agg_metrics, item_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_it\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_it\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(agg_metrics)\n",
      "File \u001b[1;32mc:\\Users\\m_sam\\anaconda3\\envs\\ts-venv\\lib\\site-packages\\gluonts\\evaluation\\_base.py:305\u001b[0m, in \u001b[0;36mEvaluator.__call__\u001b[1;34m(self, ts_iterator, fcst_iterator, num_series)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# If all entries of a target array are NaNs, the resulting metric will\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# have value \"masked\". Pandas does not handle masked values correctly.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Thus we set dtype=np.float64 to convert masked values back to NaNs\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# which are handled correctly by pandas Dataframes during\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# aggregation.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m metrics_per_ts \u001b[38;5;241m=\u001b[39m metrics_per_ts\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m    298\u001b[0m     {\n\u001b[0;32m    299\u001b[0m         col: np\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m     }\n\u001b[0;32m    303\u001b[0m )\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aggregate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics_per_ts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\m_sam\\anaconda3\\envs\\ts-venv\\lib\\site-packages\\gluonts\\evaluation\\_base.py:521\u001b[0m, in \u001b[0;36mEvaluator.get_aggregate_metrics\u001b[1;34m(self, metric_per_ts)\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, (_, agg_type, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_eval_fn\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    519\u001b[0m         agg_funs\u001b[38;5;241m.\u001b[39mupdate({k: agg_type})\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mset\u001b[39m(metric_per_ts\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m agg_funs\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    523\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome of the requested item metrics are missing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;66;03m# Compute the aggregation\u001b[39;00m\n\u001b[0;32m    526\u001b[0m totals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregation_strategy(\n\u001b[0;32m    527\u001b[0m     metric_per_ts\u001b[38;5;241m=\u001b[39mmetric_per_ts, agg_funs\u001b[38;5;241m=\u001b[39magg_funs\n\u001b[0;32m    528\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Some of the requested item metrics are missing."
     ]
    }
   ],
   "source": [
    "# 1) Best HP que obtuviste\n",
    "best_config = {\n",
    "    \"lr\":          0.0001954017201375677,\n",
    "    \"epochs\":     30,\n",
    "    \"num_layers\":  1,\n",
    "    \"hidden_size\": 40,\n",
    "}\n",
    "\n",
    "# 2) Define y entrena el estimador con esos HP\n",
    "estimator = DeepAREstimator(\n",
    "    freq=FREQ,\n",
    "    prediction_length=PREDICTION_LENGTH,\n",
    "\n",
    "    # Arquitectura y LR\n",
    "    num_layers=best_config[\"num_layers\"],\n",
    "    hidden_size=best_config[\"hidden_size\"],\n",
    "    lr=best_config[\"lr\"],\n",
    "\n",
    "    distr_output=StudentTOutput(),\n",
    "    scaling=True,\n",
    "\n",
    "    # Pasa max_epochs a través de trainer_kwargs\n",
    "    trainer_kwargs={\"max_epochs\": best_config[\"epochs\"]},\n",
    ")\n",
    "\n",
    "# Entrena usando TODO tu train_ds\n",
    "predictor = estimator.train(train_ds)\n",
    "raw_dataset = dataset  \n",
    "# 3) Preparo test_ds recortando las últimas PREDICTION_LENGTH observaciones\n",
    "test_ds = ListDataset(\n",
    "    [\n",
    "        {\n",
    "            \"start\": item[\"start\"],\n",
    "            \"target\": item[\"target\"][:-PREDICTION_LENGTH],\n",
    "            \"item_id\": item[\"item_id\"],\n",
    "        }\n",
    "        for item in raw_dataset\n",
    "    ],\n",
    "    freq=FREQ,\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Genera predicciones sobre tu test real\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds,      # tu ListDataset de test “completo”\n",
    "    predictor=predictor,\n",
    "    num_samples=100,\n",
    ")\n",
    "forecasts = list(forecast_it)\n",
    "tss       = list(ts_it)\n",
    "\n",
    "# 4) Evalúa con MAE, RMSE, CRPS, lo que quieras\n",
    "maes = []\n",
    "for ts, fcst in zip(tss, forecasts):\n",
    "    y_true = ts[-PREDICTION_LENGTH:]\n",
    "    y_pred = np.mean(fcst.samples, axis=0)\n",
    "    maes.append(mean_absolute_error(y_true, y_pred))\n",
    "print(\"MAE final:\", np.mean(maes))\n",
    "\n",
    "# Si quieres métricas más completas:\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(ts_it, forecast_it)\n",
    "print(agg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff14a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_id  pred_tn_2020_02\n",
      "0       20001      1319.527954\n",
      "1       20002       843.283020\n",
      "2       20003       716.112183\n",
      "3       20004       574.082520\n",
      "4       20005       571.325256\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for pid, fcst in zip(pids, forecasts):\n",
    "    # fcst.mean es un array de longitud PREDICTION_LENGTH\n",
    "    # En tu caso prediction_length=2 => [ene2020_pred, feb2020_pred]\n",
    "    pred_feb = float(fcst.mean[-1])\n",
    "    results.append({\n",
    "        \"product_id\": pid,\n",
    "        \"pred_tn_2020_02\": pred_feb\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.head())\n",
    "\n",
    "# Escríbelo donde quieras\n",
    "results_df.to_csv(\"../data/pronostico_feb2020_deepar_agg_V3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
